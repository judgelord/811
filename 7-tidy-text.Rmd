---
title: "Tidy text analysis"
author: "Devin Judge-Lord"
 # output:
 #   html_document:
 #   toc: true
output:
  xaringan::moon_reader:
    lib_dir: libs
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"
    css: "templates/xaringan-themer.css"
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
source(here::here("setup.R"))
```


## 1. Counting
![](https://media.giphy.com/media/3o7TKu5aIDY4tU3SXm/giphy.gif)

--

- text features $\in$\{all* words, some words, phrases, etc.\} 

---

## 1. Counting

- text features $\in$\{all* words, some words, phrases, etc.\} 

## 2. Matching
![](Figs/same.jpg)

--

- Exactly* the same string ("regular expressions", text reuse)

---

## 1. Counting
- text features $\in$\{all* words, some words, phrases, etc.\} 

## 2. Matching
- Exactly* the same string ("regular expressions", text reuse)

## 3. Classifying
![](Figs/different.jpg)

---

## 1. Counting
- text features $\in$\{all* words, some words, phrases, etc.\}

## 2. Matching
- Exactly* the same string ("regular expressions", text reuse)

## 3. Classifying
- Rules vs. probability

---

## Resources 
- [Tidy text class by Andrew Heiss](https://datavizf18.classes.andrewheiss.com/class/11-class/)
- [`tidytext`](https://cran.r-project.org/web/packages/tidytext/tidytext.pdf) package
- [Tidy text analysis](https://www.tidytextmining.com/) including
[topic modeling](https://www.tidytextmining.com/topicmodeling.html) and
[tidy() for Structural Topic Models](https://juliasilge.github.io/tidytext/reference/stm_tidiers.html) from the `stm` package. More [here](https://rdrr.io/cran/tidytext/man/stm_tidiers.html).

## Reading 
- [Introduction to cluster analysis](https://eight2late.wordpress.com/2015/07/22/a-gentle-introduction-to-cluster-analysis-using-r/)
- Tidy Natural Language Processing with [`cleanNLP`](https://scholarship.richmond.edu/cgi/viewcontent.cgi?article=1195&context=mathcs-faculty-publications)
- [Text from audio](https://www.cambridge.org/core/journals/political-analysis/article/testing-the-validity-of-automatic-speech-recognition-for-political-text-analysis/E375085D96331A47E810C01AA6DB0A46)
- Here is an [ok blog post on visualizing qualitative data](https://depictdatastudio.com/how-to-visualize-qualitative-data/)--lmk if you find a better resource. 

---

## Cool applications:
- [Gender tropes in film](https://pudding.cool/2017/08/screen-direction/)
- [Analysis of Trump's tweets confirms he writes only the (angrier) Android half](http://varianceexplained.org/r/trump-tweets/)
- [Every time Ford and Kavanaugh dodged a question, in one chart](https://www.vox.com/policy-and-politics/2018/9/28/17914308/kavanaugh-ford-question-dodge-hearing-chart)

---

class: inverse

# Counting things in fancy ways.

--

With tidy text, counting words or phrases is simple:

- `unnest_tokens()` splits each response into tokens (by word by default, but we can also tokenize by phrases of length n, called n-grams).

- [optional] `anti_join(stop_words)` removes words that often have little meaning, like "a" and "the", called stop words. We can also do this with with `filter(!(word %in% stop_words$word))`

- `count()` how many times each word appears (`count(word)` is like `group_by(word) %>% summarize(n = n()) %>% ungroup()` )

---

## Word frequency

Responses to ANES question "What do you dislike about [Democrats/Republicans]?" (V161101, V161106)
```{r ANES-data}
load(here("data/ANESdislikes.Rdata"))
d <- ANESdislikes
d[1,]
```

Tokenize by word
```{r ANES-tokens}
words <-  unnest_tokens(d, word, response)
head(words)
```

---

```{r count, cache = FALSE}
words %<>%
  ## Two ways to replace meaningless words: 
  ## 1. anti-join() a data frame with a column of words you don't want called 'word'
  anti_join(stop_words) %>% 
  ## 2. Simply filter out a strings you don't want separated with "|" (the regular expression for "or")
  filter(!str_detect(word, "people|just|dont|like|about|democrat.|republican.|party|[0-9]") ) %>%
  group_by(question) %>%
  ## Count the number of times each word occurs in each group
  count(word) 
head(words)
```

---

```{r ANESfrequency, fig.height=4, fig.width=6}
top_n(words, 10) %>% ## Top 10 words in each group
  ggplot( aes(x = reorder(word, n), y = n) ) +
  geom_col() + 
  coord_flip() +
  facet_wrap("question", scales = "free_y", strip.position="top") + 
  labs(x = "Word", y = "Count")
```

---

```{r ANES-wordcloud, eval=FALSE}
words %>%
  filter(question == "dislike_about_GOP") %>% 
  with(wordcloud(word, n, max.words = 50))
```

![GOP](Figs/ANES-wordcloud-2.png)

---

Word clouds only show word frequency, and font size is hard to visually compare. Nevertheless, [they may be useful](https://www.vis4.net/blog/2015/01/when-its-ok-to-use-word-clouds/) if all you care about is frequency.

---

Tokenize by word pair ("bi-gram")
```{r ANES-tokens-b}
## Name the new column d$bigrams ("ngrams" where n = 2)
bigrams <-  unnest_tokens(d, bigram, response, token = "ngrams", n = 2) 
head(bigrams)
```

```{r count-b, cache = FALSE}
## Count the number of times each bigram occurs in each group
bigrams %<>%
  group_by(question) %>%
  count(bigram) 
```

---

```{r ANESfrequency-b, fig.height=4, fig.width=6}
## Top 10 bigrams in each group
top_n(bigrams, 10) %>% 
  ggplot( aes(x = reorder(bigram, n), y = n) ) +
  geom_col() + 
  coord_flip() +
  facet_wrap("question", scales = "free_y", strip.position="top") + 
  labs(x = "Bigram", y = "Count")
```

---

class: center inverse

![](https://media.giphy.com/media/14kkd3gt5FJt3a/giphy.gif)
